{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "Attached you find a randomly sampled extract of some of our policies.\n",
    "We want to be able to predict the \"number_of_payment_faults\" \n",
    "(this is the number of payment attempts which failed when we try to charge the money for the policy from the customer) in the future for each new policy.\n",
    "There is a small legend in the document to describe some of the fields.\n",
    "If there are open questions about the data you can't answer by yourself do not hesitate to approach us!\n",
    "Feel free to use whatever tool or language you feel comfortable with to solve the task.\n",
    "\n",
    "## Task\n",
    "Create a statistical model to predict the \"number_of_payment_faults\" for a policy.\n",
    "There is no fixed or expected outcome what we want to see from you. \n",
    "We prefer quality over speed. \n",
    "It does not only matter if your solution yields correct results, but also your overall project structure, tools used, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import math\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype\n",
    "\n",
    "import numpy as np\n",
    "from numpy import inf\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, norm\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import KFold, cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, Lasso, BayesianRidge, LassoLarsIC\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, RobustScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from hyperopt import hp, tpe, STATUS_OK, Trials\n",
    "from hyperopt.fmin import fmin\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n",
    "\n",
    "#from subprocess import check_output\n",
    "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\")) #check the files available in the directory\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_friday = pd.read_excel('Your_Dataset.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_friday.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_friday.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_friday[df_friday['number_of_payment_faults'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_friday[df_friday['number_of_payment_faults'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['number_of_payment_faults'].isnull()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA and Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_friday.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_friday.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_friday.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Missing Data\n",
    "\n",
    "#### Check percentage of missing data\n",
    "\n",
    "If more than 50% of the data is missing, we should delete the corresponding variable and pretend it never existed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_train.isnull().sum()/df_train.count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "# missing_data.head(20)\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 12))\n",
    "plt.xticks(rotation='90')\n",
    "sns.barplot(x=percent.index, y=percent)\n",
    "plt.xlabel('Features', fontsize=15)\n",
    "plt.ylabel('Percent of missing values', fontsize=15)\n",
    "plt.title('Percent missing data by feature', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since most of the observations don't have information of 'comprehensive_product', I delete this field.\n",
    "df_train.drop('comprehensive_product', axis=1, inplace=True)\n",
    "df_test.drop('comprehensive_product', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'sf_class_tpl' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['sf_class_tpl'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['sf_class_tpl'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['sf_class_tpl'].isnull()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'sf_class_fc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['sf_class_fc'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['sf_class_fc'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['sf_class_fc'].isnull()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'policy_start'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['policy_start'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['policy_start'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['policy_start'].isnull()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'tariff_type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['tariff_type'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['tariff_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['tariff_type'].isnull()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'type_of_insurance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['type_of_insurance'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['type_of_insurance'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['type_of_insurance'].isnull()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'payment_interval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['payment_interval'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['payment_interval'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['payment_interval'].isnull()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'insured_parties'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['insured_parties'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['insured_parties'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['insured_parties'].isnull()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'profession_group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['profession_group'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['profession_group'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['profession_group'].isnull()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'age_insured_person'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_numerical_variables(df_friday, var_name):\n",
    "    print('-------------')\n",
    "    print(var_name)\n",
    "    print('\\n')\n",
    "    \n",
    "    print(df_friday[var_name].describe())\n",
    "    \n",
    "    sns.distplot(df_friday[var_name])\n",
    "    \n",
    "    #skewness and kurtosis\n",
    "    print(\"Skewness: %f\" % df_friday[var_name].skew())\n",
    "\n",
    "    df_friday[[var_name]].boxplot()\n",
    "    pyplot.show()\n",
    "\n",
    "    df_friday[[var_name]].hist()\n",
    "    pyplot.show()\n",
    "\n",
    "    print('mean: {} median: {}'.format(df_friday[var_name].mean(), df_friday[var_name].median()))\n",
    "\n",
    "    df_friday[df_friday[var_name].isnull()].shape\n",
    "\n",
    "    sns.distplot(df_friday[var_name], fit=norm)\n",
    "    fig = plt.figure()\n",
    "\n",
    "    # https://matplotlib.org/mpl-probscale/tutorial/closer_look_at_viz.html\n",
    "    res = stats.probplot(df_friday[var_name], plot=plt)\n",
    "    \n",
    "    num_null = df_friday[df_friday[var_name].isnull()].shape[0]\n",
    "    print('number of null rows: {}'.format(num_null))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_numerical_variables(df_train, 'age_insured_person')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'fc_deductible'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_numerical_variables(df_train, 'fc_deductible')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'pc_deductible'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_numerical_variables(df_train, 'pc_deductible')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'car_age_at_purchase'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_numerical_variables(df_train, 'car_age_at_purchase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'car_age_contract_start'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_numerical_variables(df_train, 'car_age_contract_start')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'annual_mileage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_numerical_variables(df_train, 'annual_mileage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Transforming some numerical variables that are really categorical\n",
    "    - 'risk_predictor_zip_code'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['risk_predictor_zip_code'] = df_train['risk_predictor_zip_code'].apply(str)\n",
    "df_test['risk_predictor_zip_code'] = df_test['risk_predictor_zip_code'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Add new column '_na' for all the numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle the missing values: \n",
    "# - create a col_NA column to indicate which row has NAs.\n",
    "def add_col_na(df_train, target):\n",
    "    for col in df_train.columns:\n",
    "        if col != target and is_numeric_dtype(df_train[col]):\n",
    "            col_vals = df_train[col]\n",
    "            if sum(col_vals.isnull()) != 0:\n",
    "                df_train[col+'_na'] = col_vals.isnull()\n",
    "                df_test[col+'_na'] = df_test[col].isnull()                \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = add_col_na(df_train, 'number_of_payment_faults')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Skewness\n",
    "\n",
    "\n",
    "Transform the skewed numeric features by taking log(feature + 1). This will make the features more normal.\n",
    "\n",
    "https://www.kaggle.com/humananalog/xgboost-lasso/code\n",
    "\n",
    "Other approaches:\n",
    "\n",
    "Box Cox Transformation of (highly) skewed features\n",
    "\n",
    "https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['fc_deductible', 'pc_deductible', 'car_age_at_purchase', \n",
    "                    'car_age_contract_start', 'annual_mileage', 'risk_predictor_zip_code'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed = df_train[numeric_features].apply(lambda x: skew(x.dropna().astype(int)))\n",
    "skewed = skewed[skewed > 0.75]\n",
    "skewed = skewed.index\n",
    "\n",
    "df_train[skewed] = np.log1p(df_train[skewed])\n",
    "df_test[skewed]  = np.log1p(df_test[skewed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Outliers\n",
    "\n",
    "Some outliers could be removed, however outliers removal is note always safe. \n",
    "I decided not to delete any of the outliers, because I found there are many outliers in the dataset.\n",
    "On the other hand, as indicated by https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n",
    "removing all outliers may affect badly the models if ever there were also outliers in the test data. That's why , instead of removing them all, we will just manage to make some of our models robust on them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Scale numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['fc_deductible', 'pc_deductible', 'car_age_at_purchase', \n",
    "                    'car_age_contract_start', 'annual_mileage'] \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[numeric_features])\n",
    "\n",
    "scaled = scaler.transform(df_train[numeric_features])\n",
    "for i, col in enumerate(numeric_features):\n",
    "    df_train[col] = scaled[:, i]\n",
    "    \n",
    "scaled = scaler.transform(df_test[numeric_features])\n",
    "for i, col in enumerate(numeric_features):\n",
    "    df_test[col] = scaled[:, i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### One-hot encode categorical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['sf_class_tpl', 'sf_class_fc', 'policy_start', 'tariff_type', 'type_of_insurance', \n",
    "                        'payment_interval', 'insured_parties', 'profession_group', 'risk_predictor_zip_code'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://queirozf.com/entries/one-hot-encoding-a-feature-on-a-pandas-dataframe-an-example\n",
    "#https://stackoverflow.com/questions/41335718/keep-same-dummy-variable-in-training-and-testing-data\n",
    "dataset = pd.concat(objs=[df_train, df_test], axis=0)\n",
    "\n",
    "for categorical_feature in categorical_features:\n",
    "    dataset = pd.concat([dataset, \n",
    "                         pd.get_dummies(dataset[categorical_feature], prefix=categorical_feature, dummy_na=True)], \n",
    "                        axis=1)\n",
    "    \n",
    "    # now drop the original 'country' column (you don't need it anymore)\n",
    "    dataset.drop([categorical_feature], axis=1, inplace=True)\n",
    "\n",
    "train_len = len(df_train)\n",
    "df_train = dataset[:train_len]\n",
    "df_test = dataset[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Convert boolean features to integer\n",
    "We could also convert boolean features with one-hot encoding, if we want to us linear regression, neural networks, svm as regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_feature_columns = df_train.dtypes[df_train.dtypes == \"bool\"].index\n",
    "bool_feature_columns.values\n",
    "\n",
    "print(bool_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bool_feature_col in bool_feature_columns:\n",
    "    df_train[bool_feature_col] = df_train[bool_feature_col].astype('int32')\n",
    "    df_test[bool_feature_col] = df_test[bool_feature_col].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Log-transformation of the target variable\n",
    "\n",
    "As (linear) models work well on normally distributed data, we need to transform the variable and make it more normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/humananalog/xgboost-lasso/code\n",
    "# https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n",
    "\n",
    "df_train['number_of_payment_faults'] = df_train['number_of_payment_faults'].astype('int32')\n",
    "\n",
    "df_target = pd.DataFrame(index = df_train.index, columns=['number_of_payment_faults'])\n",
    "df_target['number_of_payment_faults'] = np.log1p(df_train['number_of_payment_faults'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Remove 'contract_nr'\n",
    "'contract_nr' will not be used as a feature to train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_contract_nr = pd.DataFrame(index = df_train.index, columns=['contract_nr']) \n",
    "df_train_contract_nr['contract_nr'] = df_train['contract_nr']\n",
    "\n",
    "df_test_contract_nr  = pd.DataFrame(index = df_test.index, columns=['contract_nr'])\n",
    "df_test_contract_nr['contract_nr'] = df_test['contract_nr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(['contract_nr'], axis=1, inplace=True)\n",
    "df_test.drop(['contract_nr'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set size:\", df_train.shape)\n",
    "print(\"Training target set size:\", df_target.shape)\n",
    "print(\"Test set size:\", df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### TODO: Remove highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(15, 15))\n",
    "#ax = sns.heatmap(df_train.corr(), vmax=.8, square=True, fmt='.2f', annot=True, linecolor='white', linewidths=0.01)\n",
    "#plt.title('Cross correlation between variables')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation map to see how features are correlated with SalePrice\n",
    "corrmat = df_train.corr()\n",
    "plt.subplots(figsize=(12,9))\n",
    "sns.heatmap(corrmat, vmax=0.9, square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelling\n",
    "\n",
    "**Reference**\n",
    "\n",
    "https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n",
    "\n",
    "https://xgboost.readthedocs.io/en/release_0.72/tutorials/model.html\n",
    "\n",
    "https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d\n",
    "\n",
    "https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
